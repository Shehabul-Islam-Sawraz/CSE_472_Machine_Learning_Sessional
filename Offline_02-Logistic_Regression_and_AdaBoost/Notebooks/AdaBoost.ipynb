{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaBoost Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from LogisticRegression.ipynb\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "%run LogisticRegression.ipynb\n",
    "import import_ipynb\n",
    "from LogisticRegression import logistic_regression, predict, normalize, sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Precision to avoid division by 0\n",
    "EPS = 1E-12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adaptive_boosting(X, y, num_of_estimators):\n",
    "    num_samples, num_features = X.shape\n",
    "    \n",
    "    # Initialize local variables\n",
    "    # ð°, a vector of N example weights,initially 1/ð‘\n",
    "    # np.full: NumPy function that creates a new array with a \n",
    "    # specified shape and fills it with a specified value.\n",
    "    example_weights = np.full((num_samples), 1/num_samples)\n",
    "    # ð¡, a vector of K(num_of_estimators) hypothesis\n",
    "    hypothesis = []\n",
    "    # ð³, a vector of K(num_of_estimators) hypothesis weights\n",
    "    hypothesis_weights = []\n",
    "    \n",
    "    print('fitting ' + str(num_of_estimators) + ' models')\n",
    "    for k in range(num_of_estimators):\n",
    "        # Resample input examples\n",
    "        examples = np.concatenate((X, y), axis=1)\n",
    "        # replace=True: This parameter allows sampling with \n",
    "        # replacement, meaning the same element can be chosen \n",
    "        # multiple times.\n",
    "        # p=example_weights: example_weights is likely an array \n",
    "        # of weights associated with each example, indicating the \n",
    "        # probability of selecting each example.\n",
    "        data = examples[np.random.choice(num_samples, size=num_samples, replace=True, p=example_weights)]\n",
    "        \n",
    "        data_X = data[:, :num_features]\n",
    "        data_y = data[:, -1:]\n",
    "        \n",
    "        # Getting hypothesis from a weak learning algorithm\n",
    "        w = logistic_regression(\n",
    "            data_X, \n",
    "            data_y, \n",
    "            epochs=1000, \n",
    "            learning_rate=0.01, \n",
    "            early_stopping_threshold=0\n",
    "        )\n",
    "        \n",
    "        # Predicting target values with hypothesis\n",
    "        y_predicted = predict(X, w)\n",
    "        \n",
    "        # Printing accuracy of hypothesis\n",
    "        print(\"Accuracy: \")\n",
    "        print(np.sum(y == y_predicted) / num_samples)\n",
    "        \n",
    "        # Calculating error for hypothesis\n",
    "        # Check if error is too high\n",
    "        error = 0\n",
    "        for i in range(num_samples):\n",
    "            error += (example_weights[i] if y[i] != y_predicted[i] else 0)\n",
    "        \n",
    "        if error > 0.5:\n",
    "            continue\n",
    "        else:\n",
    "            hypothesis.append(w)\n",
    "            \n",
    "        if error == 0:\n",
    "            error = EPS\n",
    "        \n",
    "        # Updating example_weights\n",
    "        for i in range(num_samples):\n",
    "            example_weights[i] = example_weights[i] * ((error / (1 - error)) if y[i] == y_predicted[i] else 1)\n",
    "        \n",
    "        # Normalize example_weights\n",
    "        example_weights /= np.sum(example_weights)\n",
    "        \n",
    "        # Updating hypothesis_weights\n",
    "        hypothesis_weights.append(np.log((1 - error) / error))\n",
    "    \n",
    "    return hypothesis, np.array(hypothesis_weights).reshape(len(hypothesis), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_majority(X, hypothesis, hypothesis_weights):\n",
    "    num_samples = X.shape[0]\n",
    "    num_hypotheses = len(hypothesis)\n",
    "    \n",
    "    # Normalizing inputs X\n",
    "    X = normalize(X)\n",
    "    \n",
    "    X = np.concatenate((X, np.ones((num_samples, 1))), axis=1)\n",
    "    \n",
    "    # Calculating hypotheses\n",
    "    y_predicts = []\n",
    "    \n",
    "    for i in range(num_hypotheses):\n",
    "        y_predicted = (1 + sigmoid(np.dot(X, hypothesis[i]))) / 2\n",
    "        y_predicts.append([1 if y_pred >= 0.5 else -1 for y_pred in y_predicted])\n",
    "        \n",
    "    y_predicts = np.array(y_predicts)\n",
    "    \n",
    "    # Calculating weighted majority hypothesis and storing predictions\n",
    "    weighted_majority_hypothesis = np.dot(y_predicts.T, hypothesis_weights)\n",
    "    predictions = [1 if y_pred >= 0 else 0 for y_pred in weighted_majority_hypothesis]\n",
    "    \n",
    "    return np.array(predictions).reshape(num_samples, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preformance_matrix(y_true, y_predicted):\n",
    "    num_samples = y_true.shape[0]\n",
    "    \n",
    "    # Initializing confusion matrix values\n",
    "    TP = 0\n",
    "    TN = 0\n",
    "    FP = 0\n",
    "    FN = 0\n",
    "    \n",
    "    # calculating and storing confusion matrix outcomes\n",
    "    for i in range(num_samples):\n",
    "        if y_true[i] == 0:\n",
    "            if y_true[i] == y_predicted[i]:\n",
    "                TN += 1\n",
    "            else:\n",
    "                FP += 1\n",
    "        elif y_true[i] == 1:\n",
    "            if y_true[i] == y_predicted[i]:\n",
    "                TP += 1\n",
    "            else:\n",
    "                FN += 1\n",
    "    \n",
    "    # Calculating and storing Performance Measures\n",
    "    accuracy = (TP + TN) / (TP + FN + TN + FP)\n",
    "    precision = TP / (TP + FP)\n",
    "    recall = TP / (TP + FN)\n",
    "    specificity = TN / (TN + FP)\n",
    "    false_discovery_rate = FP / (TP + FP)\n",
    "    f1_score = 2 * recall * precision / (recall + precision)\n",
    "    \n",
    "    return (accuracy, recall, specificity, precision, false_discovery_rate, f1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Telco Customer Churn Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
